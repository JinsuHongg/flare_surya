data:
  sdo_data_root_path: /nobackupnfs1/sroy14/processed_data/Helio/nc
  train_data_path: ../data/surya-bench_sdo.csv
  valid_data_path: ../data/surya-bench_sdo.csv
  train_flare_data_path: ../data/surya-bench-flare-forecasting/train.csv
  valid_flare_data_path: ../data/surya-bench-flare-forecasting/validation.csv
  test_flare_data_path: ../data/surya-bench-flare-forecasting/test.csv
  channels: ['aia94', 'aia131', 'aia171', 'aia193', 'aia211', 'aia304', 'aia335', 'aia1600', 'hmi_m', 'hmi_bx', 'hmi_by', 'hmi_bz', 'hmi_v']
  time_delta_input_minutes: [-60,0]
  time_delta_target_minutes: +60
  n_input_timestamps: 2 # must be equal to time_delta_input_minutes
  batch_size: 2
  num_data_workers: 4
  pin_memory: True
  prefetch_factor: 2
  pooling: 1
  random_vert_flip: false
  scalers_path: ../data/scalers.yaml
  class_names: ["FL", "NFL"]

backbone:
  img_size: 4096
  patch_size: 16
  embed_dim: 1280
  depth: 10
  n_spectral_blocks: 2
  num_heads: 16
  mlp_ratio: 4.0
  drop_rate: 0.0
  window_size: 2
  dp_rank: 4
  learned_flow: false
  use_latitude_in_learned_flow: false
  rpe: false
  ensemble: null
  finetune: True # to get tokkens from attention blocks
  time_embedding:
    type: linear
    time_dim: 2 # len(config["data"]["time_delta_input_minutes"])
  init_weights: false
  checkpoint_layers: [0,1,2,3,4,5,6,7,8,9]
  nglo: 0
  dtype: float32
  freeze_backbone: True
  weight_path: ../data/Surya-1.0/surya.366m.v1.pt

head:
  type: mlp
  token_type: avg_pooling
  hyper_parameters:
    in_feature: {"cls_token": 1280, "avg_pooling": 1280, "max_pooling": 1280}
    hidden_channels: [1024, 256, 1] #[256, 64, 1]
    norm_layer: LayerNorm
    activation_layer: GELU
    bias: true
    dropout: 0.1
  threshold: 0.5 # for thresholding logit
  log_step_size: 100 # for logging score metrics every n step in training


classifier_metrics:
  task: 'binary'  # ["binary", "multiclass", "multilabel"]
  num_classes: 2
  average: 'macro'  # ["micro", "macro", "weighted", "none"]
  train_metrics_config:
    metrics_to_report: ['accuracy', 'precision', 'recall', 'f1']
  validation_metrics_config:
    metrics_to_report: ['accuracy', 'precision', 'recall', 'f1']

optimizer:
  type: Adam
  hyper_parameters:
    lr: 0.0001
    weight_decay: 0.002
  warm_up_steps: 0 #2000
  learning_rate: 0.0001  # Can use higher learning rate with LoRA
  min_lr: 0.00001

scheduler:
  type: ReduceLROnPlateau

etc:
  ckpt_dir: ../results/check_point/
  ckpt_file_name: surya_flare
  save_score_path: ../results/
  precision: 32 #16-mixed
  accelerator: gpu
  devices: [0, 1, 2, 3]
  strategy: auto
  max_epochs: 100
  log_every_n_steps: 100
  limit_train_batches: 5000
  limit_val_batches: 1000

wandb:
  save_dir: ../results/wandb
  entity: gsu-dmlab
  project: helio-fm
  log_model: False
  notes: "simple mlp head"
  offline: True
  save_code: False
  tag: ["surya", "mlp", "binary"] # backbone model, head type, task


use_latitude_in_learned_flow: false
pretrained_path: ../assets/surya.366m.v1.pt
# from_checkpoint: <PATH TO BEST CHECKPOINT linembed-input1_0.1>
loss_weights: [] # [] or list of weights per channel
rollout_steps: 0
num_mask_aia_channels: 0
drop_hmi_probability: 0.0
validate_after_epoch: 1
wandb_log_train_after: 100 # This should be less than iters_per_epoch_train
wandb_project: 'helio-fm' # Typical choices: "helio-fm", "spectformer_with_metrics"
visualization_samples: 3
save_wt_after_iter: 1000
path_experiment: checkpoints
iters_per_epoch_train: 2000
iters_per_epoch_valid: 200
iters_grad_accum: 1
parallelism: 'ddp' # Valid options: "ddp" and "fsdp" 